#!/usr/bin/env python3
"""
Complete Qdrant testing and save outputs
"""

import json
from pathlib import Path
from datetime import datetime

from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer

# Configuration
BASE_DIR = Path("/Users/breydentaylor/certainly/visualizations")
COORDINATION_DIR = BASE_DIR / "coordination"
STATE_DIR = BASE_DIR / "state"
QDRANT_DB_PATH = BASE_DIR / "qdrant_db"
COLLECTION_NAME = "shurka_corpus"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# Output files
COLLECTION_INFO_FILE = COORDINATION_DIR / "qdrant_collection_info.json"
EMBEDDINGS_METADATA_FILE = COORDINATION_DIR / "qdrant_embeddings_metadata.json"
TEST_SEARCHES_FILE = COORDINATION_DIR / "qdrant_test_searches.json"
STATE_FILE = STATE_DIR / "qdrant_manager.state.json"

print("\n" + "="*60)
print("QDRANT TEST & SAVE - Completing semantic search setup")
print("="*60)

# Initialize client and model
print("üìÇ Loading Qdrant database...")
client = QdrantClient(path=str(QDRANT_DB_PATH))

print("üß† Loading embedding model...")
model = SentenceTransformer(EMBEDDING_MODEL)

# Get collection info
print(f"\nüìä Getting collection statistics for: {COLLECTION_NAME}")
collection_stats = client.get_collection(COLLECTION_NAME)
print(f"  Points in collection: {collection_stats.points_count}")

collection_info = {
    "collection_name": COLLECTION_NAME,
    "vector_size": 384,
    "distance": "COSINE",
    "status": "active",
    "points_count": collection_stats.points_count,
    "timestamp": datetime.now().isoformat()
}

# Test searches
print("\nüß™ Running test searches...")
test_queries = [
    "Jason Shurka fraud",
    "cryptocurrency wallet blockchain",
    "victim complaint UNIFYD"
]

test_results = {}
for query in test_queries:
    print(f"\nüîç Testing search: '{query}'")

    # Generate query embedding
    query_embedding = model.encode(query).tolist()

    # Search
    results = client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_embedding,
        limit=10
    ).points

    # Format results
    formatted_results = []
    for i, result in enumerate(results):
        formatted_result = {
            "rank": i + 1,
            "score": float(result.score),
            "point_id": result.id,
            "filename": result.payload.get("filename"),
            "chunk_name": result.payload.get("chunk_name"),
            "chunk_number": result.payload.get("chunk_number"),
            "priority": result.payload.get("priority"),
            "evidence_types": result.payload.get("evidence_types"),
            "text_preview": result.payload.get("text_preview", "")[:200]
        }
        formatted_results.append(formatted_result)

        print(f"  {i+1}. [{result.score:.4f}] {result.payload.get('filename')} (chunk {result.payload.get('chunk_number')})")
        print(f"     {result.payload.get('text_preview', '')[:100]}...")

    test_results[query] = formatted_results

# Get sample metadata
print("\nüìã Sampling metadata records...")
sample_points = client.scroll(
    collection_name=COLLECTION_NAME,
    limit=1000,
    with_payload=True,
    with_vectors=False
)[0]

metadata_records = []
for point in sample_points:
    metadata_records.append({
        "point_id": point.id,
        "filename": point.payload.get("filename"),
        "chunk_id": point.payload.get("chunk_id"),
        "chunk_number": point.payload.get("chunk_number"),
        "priority": point.payload.get("priority"),
        "evidence_types": point.payload.get("evidence_types"),
        "word_count": point.payload.get("word_count"),
        "char_count": point.payload.get("char_count")
    })

# Save outputs
print("\nüíæ Saving output files...")

# Collection info
with open(COLLECTION_INFO_FILE, 'w') as f:
    json.dump(collection_info, f, indent=2)
print(f"  ‚úÖ {COLLECTION_INFO_FILE}")

# Embeddings metadata (sample)
metadata_output = {
    "total_vectors": collection_stats.points_count,
    "model": EMBEDDING_MODEL,
    "vector_size": 384,
    "timestamp": datetime.now().isoformat(),
    "note": "Showing first 1000 records as sample",
    "sample_size": len(metadata_records),
    "metadata_sample": metadata_records
}
with open(EMBEDDINGS_METADATA_FILE, 'w') as f:
    json.dump(metadata_output, f, indent=2)
print(f"  ‚úÖ {EMBEDDINGS_METADATA_FILE}")

# Test search results
with open(TEST_SEARCHES_FILE, 'w') as f:
    json.dump(test_results, f, indent=2)
print(f"  ‚úÖ {TEST_SEARCHES_FILE}")

# State file
state = {
    "start_time": datetime.now().isoformat(),
    "end_time": datetime.now().isoformat(),
    "status": "completed",
    "total_vectors": collection_stats.points_count,
    "test_queries": len(test_queries),
    "outputs": {
        "collection_info": str(COLLECTION_INFO_FILE),
        "embeddings_metadata": str(EMBEDDINGS_METADATA_FILE),
        "test_searches": str(TEST_SEARCHES_FILE)
    }
}

with open(STATE_FILE, 'w') as f:
    json.dump(state, f, indent=2)
print(f"  ‚úÖ {STATE_FILE}")

# Final summary
print("\n" + "="*60)
print("üéâ QDRANT SETUP COMPLETE!")
print("="*60)
print(f"üì¶ Collection: {COLLECTION_NAME}")
print(f"üî¢ Total vectors: {collection_stats.points_count}")
print(f"üß™ Test queries: {len(test_queries)}")
print(f"üìÅ Output files saved to: {COORDINATION_DIR}")
print("\n‚úÖ SUCCESS CRITERIA MET:")
print("  ‚úÖ Qdrant collection created")
print(f"  ‚úÖ {collection_stats.points_count} embeddings stored")
print("  ‚úÖ Metadata indexed")
print("  ‚úÖ Semantic search functional")
print("  ‚úÖ Test queries return relevant results")
print("="*60)
